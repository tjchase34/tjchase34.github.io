---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
layout: archive
classes: wide
---

<!-- Custom Styles -->
<style>

	/* Sticky navigation bar */
	.masthead {
		position: fixed;
		top: 0;
		width: 100%;
		background: white;
		height: 65px;
	}

	body {
		padding-top: 65px;
	}

	.sidebar {
		top: 65px;
	}

	h1::before {
		display: block;
		content: " ";
		margin-top: -65px;
		height: 65px;
		visibility: hidden;
		pointer-events: none;
	}

	/* Sticky navigation bar end */

	/* Change sidebar settings */
	/* .sidebar {
		opacity: 0.9;
	} */

	/* .author__avatar img {
		max-width: 140px;
	} */

	/* .sidebar {
		font-size: 22px;
	} */

	/* .author__bio,
	.author__urls {
		font-size: 18px;
	} */

	.section-sep {
		margin-bottom: 5px;
		border-width: 0px 0px 2px 0px;
		border-style: solid;
		border-color: #6cb3e0;
	}

	.section-vspace-top {
		margin-top: 30px;
	}

	.vspace-top {
		margin-top: 20px;
	}

	.paper-title {
		font-weight: bold;
	}

	.paper-desc {
		margin-top: 5px;
		text-align: justify;
	}

	.paper-links {
		margin-top: 5px;
	}

	.paper-bib {
		font-size: 14px;
	}

	.paper-authors {
		font-size: 14px;
		font-style: italic;
	}

	.edu-title {
		font-weight: bold;
		margin-top: 2px;
	}

	.edu-desc {}

	.content {
		text-align: justify;
	}

	.row {
		display: -webkit-box;
		display: -ms-flexbox;
		display: flex;
		-ms-flex-wrap: wrap;
		flex-wrap: wrap;
		/*margin-right: -15px;
	margin-left: -15px;*/
	}

	.col {
		-ms-flex-preferred-size: 0;
		flex-basis: 0;
		-webkit-box-flex: 1;
		-ms-flex-positive: 1;
		flex-grow: 1;
		max-width: 100%;
	}

	.col-sm-3,
	.col-sm-4 {
		position: relative;
		width: 100%;
		min-height: 1px;
		padding-right: 15px;
		padding-left: 15px;
	}

	.col-sm-3 {
		-webkit-box-flex: 0;
		-ms-flex: 0 0 40%;
		flex: 0 0 40%;
		max-width: 40%;
	}

	.col-sm-4 {
		-webkit-box-flex: 0;
		-ms-flex: 0 0 25%;
		flex: 0 0 25%;
		max-width: 25%;
	}

	.img-fluid {
		max-width: 100%;
		height: auto;
	}
</style>

<!-- ABOUT ===================================================== -->
<div class='section-sep' id="about">
	<h1>About Me</h1>
</div>

<div class='content vspace-top'>
		I am a doctoral student in the <a href="https://droneslab.github.io/">Distributed RObotics and Networked Embedded Sensing (DRONES)</a> Lab at the University at Buffalo (UB), advised by <a href="https://engineering.buffalo.edu/computer-science-engineering/people/faculty-directory.host.html/content/shared/engineering/computer-science-engineering/profiles/faculty/dantu-karthik.detail.html">Dr. Karthik Dantu</a> (Dept. of Computer Science and Engineering), co-advised by <a href="https://engineering.buffalo.edu/mechanical-aerospace/people/faculty/j-crassidis.html">Dr. John Crassidis</a> in the <a href="https://ancs.eng.buffalo.edu/index.php/Main_Page">Advanced Navigation and Control Systems (ANCS)</a> Lab (Dept. of Mechanical and Aerospace Engineering). I am a current Pathways Student at NASA Goddard Space Flight Center (GSFC) in the <a href="https://sed.gsfc.nasa.gov/etd/587">Science Data Processing</a> branch (code 587).<br>
    <br>
    My research interests include spacecraft perception and autonomy, optical navigation systems, simultaneous localization and mapping (SLAM), embedded computing, and computer vision. My current research is focused on unsupervised, generative, and representation learning-based solutions for space-vision tasks such as visual terrain detection, scene reconstruction, and landmark recognition. In the past, I have also worked on dynamic feature reasoning, 3D feature location estimation, and sim-to-real domain adaptation. 
</div>

<!-- EDUCATION ===================================================== -->
<div class='section-sep section-vspace-top'>
	<h1>Education</h1>
</div>

<div class='row vspace-top'>
  <div class="col-sm-4">
        Aug. 2020 - Present
	</div>
  <div class="col">
    <div class='edu-title'>
      University at Buffalo
    </div>
			<div class="edu-desc">
				Ph.D. Candidate, Computer Science and Engineering<br>
        Distributed RObotics and Networked Embedded Sensing (DRONES) Lab (Dr. Karthik Dantu)<br>
        Advanced Navigation and Control Systems (ANCS) Lab (Dr. John Crassidis)<br>
			</div>
		</div>
</div>

<div class='row vspace-top'>
  <div class="col-sm-4">
        Aug. 2020 - Feb. 2023
	</div>
  <div class="col">
    <div class='edu-title'>
      University at Buffalo
    </div>
			<div class="edu-desc">
				M.S. in Computer Science and Engineering
			</div>
		</div>
</div>

<div class='row vspace-top'>
  <div class="col-sm-4">
        Aug. 2016 - May 2020
	</div>
  <div class="col">
    <div class='edu-title'>
      University at Buffalo
    </div>
			<div class="edu-desc">
				B.S. in Computer Science<br>
				Certification, Data-Intensive Computing
			</div>
		</div>
</div>

<!-- <div class='row vspace-top'>
  <div class="col-sm-4">
        Aug. 2016 - May 2020
	</div>
  <div class="col">
    <div class='edu-title'>
      University at Buffalo
    </div>
			<div class="edu-desc">
				Certificate, Data Intensive Computing
			</div>
		</div>
</div> -->

<!-- INTERNSHIPS ===================================================== -->
<div class='section-sep section-vspace-top'>
		<h1>Experience</h1>
</div>

<div class='row vspace-top'>
  <div class="col-sm-4">
    May 2018 - Present
  </div>
  <div class="col">
    <div class='edu-title'>
      NASA Goddard Space Flight Center
    </div>
    <div class="edu-desc">
      Student Researcher/Software Engineer, Science Data Processing<br>
      <!-- Embedded Autonomy and AI<br>
	  R&D Flight Software -->
    </div>
  </div>
</div>

<!-- <div class='row vspace-top'>
  <div class="col-sm-4">
    May 2018 - Dec. 2021
  </div>
  <div class="col">
    <div class='edu-title'>
      NASA Goddard Space Flight Center - Wallops Flight Facility
    </div>
    <div class="edu-desc">
      Pathways Student, Wallops Systems Software Engineering Branch (Code 589)<br>
      Cube/Small-satellite Flight Software
    </div>
  </div>
</div> -->

<div class='row vspace-top'>
  <div class="col-sm-4">
    Sep. 2019 - Jan. 2020
  </div>
  <div class="col">
    <div class='edu-title'>
      NASA Jet Propulsion Laboratory
    </div>
    <div class="edu-desc">
      Software Engineer, Intern, Mobility and Robotic Systems<br>
      <!-- Simulation, Mars 2020 Rover Operations -->
    </div>
  </div>
</div>

<div class='row vspace-top'>
  <div class="col-sm-4">
    Jan. 2019 - Jan. 2020
  </div>
  <div class="col">
    <div class='edu-title'>
      NOVI Aerospace
    </div>
    <div class="edu-desc">
      Data Scientist, Intern<br>
      <!-- Dataset Curator -->
    </div>
  </div>
</div>

<div class='row vspace-top'>
  <div class="col-sm-4">
    Mar. 2016 - May. 2020
  </div>
  <div class="col">
    <div class='edu-title'>
      UB Nanosatellite Laboratory
    </div>
    <div class="edu-desc">
      Flight Software Lead<br>
	  <!-- Three CubeSat Missions -->
    </div>
  </div>
</div>


<!-- PUBLICATIONS ===================================================== -->
<div class='section-sep section-vspace-top'>
		<h1>Featured Publications</h1>
</div>

<!-- YOCOv2 ----- -->

<!-- PIXER ----- -->

<!-- SLAM in Space ----- -->

<!-- MARs ----- -->
<div class='row vspace-top'>
		<div class="col-sm-3">
			<a href="/assets/images/web_mars.png"><img src='/assets/images/web_mars.png' class='img-fluid'></a>
		</div>
		<div class="col">
			<div class='paper-title'>
				MARs: Multi-view Attention Regularizations for Patch-based Feature Recognition of Space Terrain
			</div>
			<div class='paper-authors'>
				<u>Timothy Chase Jr</u>, Karthik Dantu
			</div>
			<div class='paper-bib'>
				European Conference on Computer Vision (ECCV), 2024
			</div>
			<div class='paper-desc'>
				The visual tracking of surface terrain is required for spacecraft to land on or navigate within close proximity to celestial objects, where current approaches rely on template matching with pre-gathered patch-based features. While recent literature has focused on in-situ detection methods, robust description is still needed. In this work, we explore metric learning as the lightweight feature description mechanism and find that current solutions fail to address inter-class similarity and multi-view observational geometry. We attribute this to the view-unaware attention mechanism and introduce Multi-view Attention Regularizations (MARs) to constrain the channel and spatial attention across multiple feature views. We analyze many modern metric learning losses with and without MARs and demonstrate improved terrain-feature recognition performance by upwards of 85%. We additionally introduce the Luna-1 dataset, consisting of Moon crater landmarks and reference navigation frames from NASA mission data to support future research.
			</div>
			<div class='paper-links'>
				<a href="https://droneslab.github.io/mars/" target="_blank">[Project Page]</a>
				<a href="https://arxiv.org/abs/2410.05182" target="_blank">[Paper]</a>
				<a href="https://github.com/droneslab/mars/" target="_blank">[Code]</a>
				<a href="https://github.com/droneslab/Luna-1/" target="_blank">[Luna-1 Dataset]</a>
				<!-- <a href="https://tjchase34.github.io/preslam_web/" target="_blank">[Project Page]</a> -->
				<!-- [Project Page (Coming Soon)] -->
			</div>
		</div>
	</div>

<!-- STO ----- -->
<div class='row vspace-top'>
		<div class="col-sm-3">
			<a href="/assets/images/web_sto.png"><img src='/assets/images/web_sto.png' class='img-fluid'></a>
		</div>
		<div class="col">
			<div class='paper-title'>
				Unsupervised Surface-to-Orbit View Generation of Planetary Terrain
			</div>
			<div class='paper-authors'>
				<u>Timothy Chase Jr</u>, Sannihith Kilaru, Shivendra Srinivas, Karthik Dantu
			</div>
			<div class='paper-bib'>
				IEEE Aerospace Conference, 2024
			</div>
			<div class='paper-desc'>
				We present a generative approach to orbital-style view synthesis that improves the visual fidelity of Inverse Perspective image tranformations (IPMs) on planetary surface terrain. We describe how to condition generative model learning on input signals given only by surface and IPM images permitting an entirely unsupervised approach. We show consistency in both feature structure and location, allowing for the mapping of auxiliary information like semantic pixel labels. Through in-depth qualitative and quantitative analysis, we demonstrate the ability of our method to create less-deformed, more realistic images that improve downstream learning tasks.
			</div>
			<div class='paper-links'>
				<a href="https://ieeexplore.ieee.org/abstract/document/10521336" target="_blank">[Paper]</a>
				<!-- <a href="https://tjchase34.github.io/preslam_web/" target="_blank">[Project Page]</a> -->
				<!-- [Project Page (Coming Soon)] -->
			</div>
		</div>
	</div>

<!-- PRESLAM ----- -->
<div class='row vspace-top'>
		<div class="col-sm-3">
			<a href="/assets/images/web_preslam.png"><img src='/assets/images/web_preslam.png' class='img-fluid'></a>
		</div>
		<div class="col">
			<div class='paper-title'>
				PRE-SLAM: Persistence Reasoning in Edge-assisted Visual SLAM
			</div>
			<div class='paper-authors'>
				<u>Timothy Chase Jr</u>, Ali J. Ben Ali, Steven Y. Ko, Karthik Dantu
			</div>
			<div class='paper-bib'>
				IEEE Conference on Mobile Ad Hoc and Smart Systems (MASS), 2022
			</div>
			<div class='paper-desc'>
				We introduce PRE-SLAM, an edge-assisted visual SLAM system that incorporates feature persistence filtering. We revisit the centralized persistence filter architecture and make a series of modifications to allow for dynamic feature filtering in an edge-assisted setting. Using two locally collected datasets, we show how our split persistence filter implementation reduces map-point and keyframe retention by 26.6% and 16.6% respectively. By filtering out dynamic map-points from the system, we demonstrate an improvement in average localization accuracy by more than 50%. We also demonstrate how incorporating feature persistence filtering into Edge-SLAM retains the key benefits and performance enhancements of an edge-assisted Visual-SLAM system, with an added communication overhead of only 500 KB while decreasing overall map size by 8.6%.
			</div>
			<div class='paper-links'>
				<a href="https://ieeexplore.ieee.org/abstract/document/9973604" target="_blank">[Paper]</a>
				<!-- <a href="https://tjchase34.github.io/preslam_web/" target="_blank">[Project Page]</a> -->
				<!-- [Project Page (Coming Soon)] -->
			</div>
		</div>
	</div>

<!-- YOCO AAS ----- -->
<div class='row vspace-top'>
		<div class="col-sm-3">
			<a href="/assets/images/yoco.png"><img src='/assets/images/yoco.png' class='img-fluid'></a>
		</div>
		<div class="col">
			<div class='paper-title'>
				You Only Crash Once: Improved Object Detection for Real-Time, Sim-to-Real Hazardous Terrain Detection and Classification for Autonomous Planetary Landings
			</div>
			<div class='paper-authors'>
				<u>Timothy Chase Jr</u>, Chris Gnam, John Crassidis, Karthik Dantu
			</div>
			<div class='paper-bib'>
				AAS/AIAA Astrodynamics Specialist Conference, 2022
			</div>
			<div class='paper-desc'>
				In this work, we introduce You Only Crash Once (YOCO), a learning-based visual hazardous terrain detection and classification technique for autonomous spacecraft planetary landings. Through the use of unsupervised domain adaptation we tailor YOCO for training by simulation, removing the need for real-world annotated data and expensive mission surveying phases. We further improve the transfer of representative terrain knowledge between simulation and the real world through visual similarity clustering. We demonstrate the utility of YOCO through a series of terrestrial and extraterrestrial simulation-to-real experiments and show substantial improvements toward the ability to both detect and accurately classify instances of planetary terrain.
			</div>
			<div class='paper-links'>
				<a href="https://arxiv.org/abs/2303.04891" target="_blank">[Paper]</a>
				<!-- <a href="https://tjchase34.github.io/yoco_web/" target="_blank">[Project Page]</a> -->
				<!-- [AAS Paper (Coming Soon)] -->
				<!-- [Project Page (Coming Soon)] -->
			</div>
		</div>
	</div>

<!-- MHT ----- -->
<div class='row vspace-top'>
		<div class="col-sm-3">
			<a href="/assets/images/mht.png"><img src='/assets/images/mht.png' class='img-fluid'></a>
		</div>
		<div class="col">
			<div class='paper-title'>
				Efficient Feature Matching and Mapping for Terrain Relative Navigation Using Hypothesis Gating
			</div>
			<div class='paper-authors'>
				Chris Gnam*, <u>Timothy Chase Jr*</u>, Karthik Dantu, John Crassidis<br>
				*Equal Contribution
			</div>
			<div class='paper-bib'>
				AIAA SciTech Forum, 2022
			</div>
			<div class='paper-desc'>
				This paper tackles the inaccuracies and inefficiencies of standard image feature matching processes on spaceflight processors, by leveraging traditional onboard navigation filter information to drastically reduce the number of matching candidates. Estimated feature location is used to form statistical prediction gates around a given feature, for which all points lying inside are treated as inliers and fed to the matching process. Using a simulated trajectory around a high-fidelity 3D asteroid model and a single monocular camera, we demonstrate an overall reduction of around 87% in average matching time for three popular feature description techniques. We showcase how feature gating substantially increases matching accuracy, giving utility towards purely monocular terrain relative navigation.
			</div>
			<div class='paper-links'>
				<a href="https://arc.aiaa.org/doi/10.2514/6.2022-2513" target="_blank">[Paper]</a>
				<a href="https://video.aiaa.org/title/2934eaa3-91fb-4897-ba28-9eb774074875" target="_blank">[Video Presentation]</a>
				<!-- <a href="https://tjchase34.github.io/mht_web/" target="_blank">[Project Page]</a> -->
				<!-- [Project Page (Coming Soon)] -->
			</div>
		</div>
	</div>

<!-- NEWS ===================================================== -->
<div class='section-sep section-vspace-top'>
		<h1>News</h1>
</div>

**2025**

**2024**

- **Nov. 2024**: One co-authored paper accepted to the 2025 AAAI Innovative Applications of Artificial Intelligence: <b><i>Applications of The NASA On-Board Artificial Intelligence Research Platform</i></b>.
- **Oct. 2024**: One co-authored paper accepted to the 2025 AAS Guidance, Navigation and Control Conference: <b><i>Machine Learning based Crater Detection for Terrain Relative Navigation</i></b>.
- **Oct. 2024**: One co-authored paper accepted to the 2025 IEEE Aerospace Conference: <b><i>Evaluation and Integration of YOLO Models for Autonomous Crater Detection</i></b>.
- **Aug. 2024**: I serve as principle investigator ($22K) of NASA research: <b><i>Terrain Modeling and Landmark Navigation with Radiance Fields</i></b>.
- **Jul. 2024**: I am a co-organizer of the <i>Image Processing and Computer Vision</i> session of the 2025 IEEE Aerospace Conference.
- **Jul. 2024**: One paper accepted to the 2024 European Conference on Computer Vision (ECCV): <b><i><a href="https://droneslab.github.io/mars/">MARs: Multi-view Attention Regularizations for Patch-based Feature Recognition of Space Terrain</a></i></b>.
- **Jun. 2024**: One co-authored paper accepted to the 2024 ESA/IAA Conference on AI in and for Space: <b><i>The Onboard Artificial Intelligence Research (OnAIR) Platform</i></b>.
- **Jun. 2024**: I serve as a summer internship mentor for four students at NASA GSFC (robotics/learning focus).

**2023**

- **Nov. 2023**: One poster presented at the 2023 Northeast Robotics Colloquium (NERC): <b><i><a href="assets/pdfs/nerc23.pdf">Enhanced Visual Perception for Autonomous Spacecraft Navigation</a></i></b>.
- **Nov. 2023**: Two papers accepted to the 2024 IEEE Aerospace Conference: <b><i>Profiling Vision-based Deep Learning Architectures on NASA SpaceCube Platforms</i></b> and <b><i>Unsupervised Surface-to-Orbit View Generation of Planetary Terrain</i></b>.
- **Sep. 2023**: I've received the NASA GSFC Smart Award for my nine-month mentorship and technical advisement of a Drexel University Dept. of Computer Science senior project.
- **Aug. 2023**: I serve as principle investigator ($27.5K) of NASA research: <b><i>Towards Learning-based Visual Perception with GAVIN: the Goddard AI Verification and INtegration Tool Suite</i></b>.
- **Aug. 2023**: One co-authored poster presented at the 2023 Small Satellite Conference (SmallSat): <b><i><a href="assets/pdfs/smallsat23.pdf">An Autonomous Agent Framework for Constellation Missions: A Use Case for Predicting Atmospheric CO2</a></i></b>.
- **Jun. 2023**: I serve as a summer internship mentor for two students at NASA GSFC (robotics focus).
- **Apr. 2023**: My contributions to STP-H9 SCENIC are <a href="https://www.instagram.com/p/CrBefvXJBvy/?img_index=1">featured by UB Engineering's Instagram</a> for National Robotics Week. UB features this as post of the month.
- **Mar. 2023**: My work on NASA mission STP-H9 SCENIC <a href="https://twitter.com/SpaceX/status/1635803956533399553?s=20">launches to the International Space Station</a>.
- **Feb. 2023**: I've received my M.S. in Computer Science and Engineering from University at Buffalo.
- **Feb. 2023**: One co-authored paper accepted to the 2023 Small Satellite Conference (SmallSat): <b><i><a href="https://digitalcommons.usu.edu/smallsat/2023/all2023/147/">NASA SpaceCube Next-Generation Artificial-Intelligence Computing for STP-H9-SCENIC on ISS</a></i></b>.

**2022**

- **Aug. 2022**: I start a NASA-led mentorship/technical advisement role to a six-student senior project group from Drexel University.
- **Jun. 2022**: One paper accepted to the 2022 IEEE International Conference on Mobile Ad Hoc and Smart Systems (MASS): <b><i><a href="https://ieeexplore.ieee.org/abstract/document/9973604">PRE-SLAM: Persistence Reasoning in Edge-assisted Visual SLAM</a></i></b>.
- **May 2022**: One paper accepted to the 2022 AAS/AIAA Astrodynamics Specialist Conference: <b><i><a href="https://arxiv.org/abs/2303.04891">You Only Crash Once: Improved Object Detection for Real-Time, Sim-to-Real Hazardous Terrain Detection and Classification for Autonomous Planetary Landings</a></i></b>.
- **May 2022**: One co-authored poster presented at the 20th ACM International Conference on Mobile Systems, Applications, and Services (MobiSys): <b><i>A Modular, Extensible Framework for Modern Visual SLAM Systems</i></b>.
- **Feb. 2022**: One co-authored paper accepted to the 2022 AAS Guidance, Navigation and Control Conference: <b><i>Attitude Determination via Earth Surface Feature Tracking Given Precise Orbit Knowledge</i></b>.
- **Sep. 2021**: One paper accepted to the 2022 AIAA SciTech Forum: <b><i><a href="https://arc.aiaa.org/doi/abs/10.2514/6.2022-2513">Efficient Feature Matching and Mapping for Terrain Relative Navigation Using Hypothesis Gating</a></i></b>.

<!-- - August 2020: I start an undergraduate teaching assistantship for CSE 421/521: Introduction to Operating Systems. -->
